{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9365cc45",
   "metadata": {},
   "source": [
    "\\begin{alignat*}{2}\n",
    "&\\! \\max_{q \\succeq 0} & \\qquad & \\sum_j p(x_j) q(x_j) \\mathbb{E}_y\\left[ \\log\\left(1 + \\frac{\\lambda}{q(x_j)} \\left( \\theta - \\rho(x_j, y, \\beta) \\right) \\right) \\right]  \\\\\n",
    "& \\text{subject to} & & \\sum_j p(x_j) q(x_j) \\leq c.\n",
    "\\end{alignat*}\n",
    "Lower bound objective.\n",
    "\\begin{align}\n",
    "& q(x_j) \\mathbb{E}_y\\left[ \\log\\left(1 + \\frac{\\lambda}{q(x_j)} \\left( \\theta - \\rho(x_j, y, \\beta) \\right) \\right) \\right] \\\\\n",
    "&\\geq q(x_j) \\mathbb{E}_y\\left[ \\frac{\\lambda}{q(x_j)} \\left( \\theta - \\rho(x_j, y, \\beta) \\right) - \\frac{\\psi(\\lambda)}{q(x_j)^2} \\left( \\theta - \\rho(x_j, y, \\beta) \\right)^2 \\right] & \\left( \\psi(\\lambda) \\doteq -\\lambda - \\log(1 - \\lambda) \\right) \\\\\n",
    "&= \\lambda \\left( \\theta - \\mathbb{E}_y\\left[\\rho(x_j, y, \\beta)\\right] \\right) - \\frac{1}{q(x_j)} \\psi(\\lambda) \\mathbb{E}_y\\left[\\left( \\theta - \\rho(x_j, y, \\beta) \\right)^2 \\right] \n",
    "\\end{align}\n",
    "Substitute\n",
    "\\begin{alignat*}{2}\n",
    "&\\! \\min_{q \\succeq 0} & \\qquad & \\sum_j \\frac{p(x_j)}{q(x_j)} \\psi(\\lambda) \\mathbb{E}_y\\left[\\left( \\theta - \\rho(x_j, y, \\beta) \\right)^2 \\right]  \\\\\n",
    "& \\text{subject to} & & \\sum_j p(x_j) q(x_j) \\leq c.\n",
    "\\end{alignat*}\n",
    "Substitute $\\phi(x_j) \\leftarrow p(x_j) q(x_j)$, \n",
    "\\begin{alignat*}{2}\n",
    "&\\! \\min_{q \\succeq 0} & \\qquad & \\sum_j \\frac{p^2(x_j)}{\\phi(x_j)} \\psi(\\lambda) \\mathbb{E}_y\\left[\\left( \\theta - \\rho(x_j, y, \\beta) \\right)^2 \\right]  \\\\\n",
    "& \\text{subject to} & & \\sum_j \\phi(x_j) \\leq c.\n",
    "\\end{alignat*}\n",
    "This has solution $\\phi(x_j) \\propto p(x_j) \\sqrt{\\mathbb{E}_y\\left[\\left( \\theta - \\rho(x_j, y, \\beta) \\right)^2 \\right]}$ which implies $q(x_j) \\propto \\sqrt{\\mathbb{E}_y\\left[\\left( \\theta - \\rho(x_j, y, \\beta) \\right)^2 \\right]}$.\n",
    "\n",
    "**Simulation**: $y \\sim \\text{Bournelli}(x_j)$, $\\rho$ is false positive rate, and $\\beta$ is classifier threshold,  \n",
    "\\begin{align}\n",
    "\\rho(x_j, y, \\beta) &= 1_{\\beta \\leq x_j} (1 - y) \\\\\n",
    "\\mathbb{E}_y\\left[ \\rho(x_j, y, \\beta) \\right] &= 1_{\\beta \\leq x_j} (1 - x_j) \\\\\n",
    "\\mathbb{E}_y\\left[ \\rho(x_j, y, \\beta)^2 \\right] &= 1_{\\beta \\leq x_j} (1 - x_j) \\\\\n",
    "\\mathbb{E}_y\\left[\\left( \\theta - \\rho(x_j, y, \\beta) \\right)^2 \\right] &= \\theta^2 + \\left(1 -  2 \\theta\\right) 1_{\\beta \\leq x_j} (1 - x_j)\n",
    "\\end{align}\n",
    "So we should anticipate\n",
    "* When $\\beta = 1$, sampling should be constant, since $1_{\\beta \\leq x_j} (1 - x_j)$ is zero everywhere.\n",
    "* When $\\beta < 1$ and $\\theta < 1/2$, sampling should be\n",
    "  * constant for $x_j < \\beta$ since $1_{\\beta \\leq x_j} (1 - x_j)$ is zero in this regime, and\n",
    "  * jump to a higher sampling rate it $x_j = \\beta$ since $1_{\\beta \\leq x_j} (1 - x_j)$ is maximized at that point, and\n",
    "  * sample at a decreasing rate as $x_j$ increases above $\\beta$ since  $1_{\\beta \\leq x_j} (1 - x_j)$ is decreasing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "102ba410-442e-49b6-a4f0-91979539755e",
   "metadata": {
    "code_folding": [
     0,
     1,
     17
    ]
   },
   "outputs": [],
   "source": [
    "class LabellingPolicyPrimalPlayer(object):\n",
    "    def __init__(self, *, policy, q_min, target_rate, theta, rho, opt, sched):\n",
    "        from IwUpperMartingale import IwUpperMartingale\n",
    "        \n",
    "        assert 0 < q_min < target_rate < 1\n",
    "        \n",
    "        super().__init__()\n",
    "        self._policy = policy\n",
    "        self._q_min = q_min\n",
    "        self._target_rate = target_rate\n",
    "        self._theta = theta\n",
    "        self._rho = rho\n",
    "        self._opt = opt\n",
    "        self._sched = sched\n",
    "\n",
    "        self._iwmart = IwUpperMartingale(rho=rho, theta=theta, q_min=q_min, n_betas=1000, alpha=0.05)\n",
    "        \n",
    "    def predict(self, PX):\n",
    "        import torch\n",
    "        \n",
    "        _, X = PX\n",
    "\n",
    "        Q = self._q_min + (1 - self._q_min) * self._policy(X).squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            cons = (self._target_rate - torch.mean(Q)).item()\n",
    "        return Q, cons\n",
    "        \n",
    "    def update(self, PX, Q, dual):\n",
    "        import torch\n",
    "        \n",
    "        P, X = PX\n",
    "\n",
    "        # TODO: torch.mean or torch.sum (?)\n",
    "        #       one component of the loss is over everything and the other is not\n",
    "\n",
    "        loss = - dual * (self._target_rate - torch.mean(Q))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            L = torch.bernoulli(Q)\n",
    "            \n",
    "        if torch.any(L == 1):\n",
    "            # TODO: can we clip curlam after observing q_min for this batch, but before observing rho?\n",
    "            #       b/c then we could handle Q > 0 without an explicit minimum\n",
    "            #       i think it's ok b/c conditioned on x the mean is still correct and then tower property\n",
    "            \n",
    "            curlam = self._iwmart.getlam()\n",
    "            curbeta, _ = self._iwmart.getbeta()\n",
    "            Qsamp = Q[L == 1]\n",
    "            Xsamp = X[L == 1,:]\n",
    "            Psamp = P[L == 1]\n",
    "            with torch.no_grad():\n",
    "                Xsampnumpy = Xsamp.numpy()\n",
    "                Ysamp = self._rho.realize((Psamp, Xsampnumpy))\n",
    "                PXYsamp = ((Psamp, Xsampnumpy), Ysamp)\n",
    "                rhosamp = torch.Tensor(self._rho(PXYsamp, curbeta))\n",
    "            f = torch.log1p(curlam * (self._theta - rhosamp) / Qsamp)\n",
    "            stopf = f.detach()\n",
    "            loss -= (torch.log(Qsamp) * stopf + f).mean()\n",
    "            \n",
    "            # update betting martingale\n",
    "            with torch.no_grad():\n",
    "                for qi, pi, xi, yi in zip(Qsamp, Psamp, Xsampnumpy, Ysamp):\n",
    "                    self._iwmart.addobs(qi.item(), ((pi, xi), yi))\n",
    "            \n",
    "        self._opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self._opt.step()\n",
    "        self._sched.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c10d3569",
   "metadata": {
    "code_folding": [
     0,
     10,
     23,
     61,
     75
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betastar = 0.683772233983162\n",
      "n    \t bet       \t beta      \t p                                                           \t Q(p)                                                        \t cons      \t dual      \t\n",
      "    0\t 1.250e-02\t 1.000e+00\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.46 0.50 0.52 0.53 0.51 0.50 0.48 0.50 0.48 0.51 0.50]\t -2.996e-01\t 5.000e-01\t\n",
      "    1\t 1.250e-02\t 1.000e+00\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.46 0.50 0.51 0.53 0.51 0.50 0.48 0.50 0.48 0.51 0.50]\t -2.972e-01\t 5.000e-01\t\n",
      "    2\t 1.250e-02\t 1.000e+00\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.46 0.50 0.51 0.52 0.50 0.49 0.48 0.50 0.48 0.51 0.50]\t -2.949e-01\t 5.000e-01\t\n",
      "    4\t 1.250e-02\t 1.000e+00\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.45 0.49 0.51 0.52 0.50 0.49 0.47 0.49 0.47 0.50 0.50]\t -2.902e-01\t 5.000e-01\t\n",
      "    8\t 1.250e-02\t 1.000e+00\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.44 0.48 0.50 0.51 0.49 0.48 0.46 0.48 0.47 0.49 0.49]\t -2.809e-01\t 5.000e-01\t\n",
      "   16\t 1.250e-02\t 1.000e+00\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.42 0.46 0.48 0.49 0.47 0.46 0.44 0.47 0.45 0.47 0.47]\t -2.624e-01\t 5.000e-01\t\n",
      "   32\t 1.250e-02\t 1.000e+00\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.39 0.43 0.44 0.45 0.43 0.42 0.41 0.43 0.41 0.44 0.43]\t -2.262e-01\t 5.000e-01\t\n",
      "   64\t 1.250e-02\t 1.000e+00\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.33 0.36 0.37 0.38 0.37 0.36 0.34 0.36 0.35 0.37 0.36]\t -1.580e-01\t 5.000e-01\t\n",
      "  128\t 1.250e-02\t 7.908e-01\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.22 0.25 0.26 0.27 0.26 0.25 0.24 0.25 0.24 0.25 0.25]\t -4.956e-02\t 5.000e-01\t\n",
      "  256\t 1.250e-02\t 7.187e-01\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.17 0.19 0.20 0.21 0.20 0.19 0.18 0.19 0.19 0.19 0.20]\t 9.252e-03\t 0.000e+00\t\n",
      "  512\t 1.250e-02\t 6.987e-01\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.17 0.19 0.20 0.21 0.20 0.19 0.18 0.19 0.19 0.19 0.20]\t 8.904e-03\t 0.000e+00\t\n",
      " 1024\t 1.250e-02\t 6.897e-01\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.17 0.19 0.20 0.21 0.20 0.19 0.18 0.19 0.19 0.20 0.20]\t 7.701e-03\t 0.000e+00\t\n",
      " 2048\t 4.362e-03\t 6.887e-01\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.17 0.18 0.20 0.20 0.19 0.19 0.18 0.20 0.19 0.19 0.19]\t 1.015e-02\t 0.000e+00\t\n",
      " 4096\t 1.063e-03\t 6.887e-01\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.17 0.18 0.20 0.20 0.19 0.19 0.18 0.20 0.19 0.20 0.19]\t 9.507e-03\t 0.000e+00\t\n",
      " 8192\t 2.573e-03\t 6.887e-01\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.14 0.15 0.17 0.17 0.16 0.16 0.15 0.17 0.17 0.17 0.16]\t 3.925e-02\t 0.000e+00\t\n",
      "16384\t 4.093e-03\t 6.887e-01\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.14 0.13 0.14 0.14 0.14 0.13 0.13 0.25 0.30 0.25 0.16]\t 2.548e-02\t 0.000e+00\t\n",
      "32768\t 3.471e-03\t 6.867e-01\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.09 0.08 0.09 0.09 0.09 0.09 0.09 0.28 0.33 0.25 0.11]\t 5.587e-02\t 0.000e+00\t\n",
      "65536\t 2.986e-03\t 6.867e-01\t [0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00]\t [0.10 0.06 0.07 0.07 0.07 0.06 0.06 0.31 0.31 0.25 0.17]\t 6.225e-02\t 0.000e+00\t\n"
     ]
    }
   ],
   "source": [
    "def test_once(gen):\n",
    "    from IwUpperMartingale import IwUpperMartingale\n",
    "    from OnlineMinimax import OnlineMinimax, BestResponse, BeTheLeader\n",
    "    from LogisticRegression import LogisticRegressor\n",
    "\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    np.set_printoptions(2, floatmode='fixed')\n",
    "    \n",
    "    class FalsePositive(object):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def __call__(self, PXY, betas):\n",
    "            (P, _), Y = PXY\n",
    "            return (betas <= P) * (1 - Y)\n",
    "\n",
    "        def realize(self, PX):\n",
    "            P, _ = PX\n",
    "            return gen.binomial(1, P)\n",
    "        \n",
    "    knots = np.linspace(0, 1, 20)\n",
    "    def featurize(p):\n",
    "        rv = [0]*len(knots)\n",
    "\n",
    "        if p >= knots[-1]:\n",
    "            rv[-1] = 1\n",
    "        else:\n",
    "            z = np.digitize(p, knots)\n",
    "            vlow, vhigh = knots[z-1], knots[z]\n",
    "            frac = (p - vlow) / (vhigh - vlow)\n",
    "            rv[z] = frac\n",
    "            rv[z - 1] = 1 - frac\n",
    "            \n",
    "        return rv\n",
    "\n",
    "    policy = LogisticRegressor(in_features=len(featurize(0.1)), out_features=1)\n",
    "    q_min = 1/40\n",
    "    target_rate = 1/5\n",
    "    theta = 1/20\n",
    "    betastar = 1 - np.sqrt(2 * theta)\n",
    "    print(f'betastar = {betastar}')\n",
    "    rho = FalsePositive()\n",
    "    opt = torch.optim.Adam(policy.parameters(), lr=5e-3)\n",
    "    sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda t:1) # (1+t/1000)**(-0.5))\n",
    "\n",
    "    primal_player = LabellingPolicyPrimalPlayer(policy=policy, \n",
    "                                                q_min=q_min,\n",
    "                                                target_rate=target_rate,\n",
    "                                                theta=theta,\n",
    "                                                rho=rho,\n",
    "                                                opt=opt,\n",
    "                                                sched=sched)\n",
    "    # anything bigger than (1 - Log[2]) should be good enough for max_dual\n",
    "    dual_player = BestResponse(max_dual=1/2)\n",
    "    # TODO: windowed be the leader, batching with best response\n",
    "    minimax = OnlineMinimax(primal_player=primal_player, dual_player=dual_player)\n",
    "\n",
    "    print(f'{\"n\":5s}\\t', f'{\"bet\":10s}\\t', f'{\"beta\":10s}\\t', f'{\"p\":60s}\\t', f'{\"Q(p)\":60s}\\t', f'{\"cons\":10s}\\t', f'{\"dual\":10s}\\t')\n",
    "    batch_size = 64\n",
    "    for n in range(1 + (1 << 16)):\n",
    "        p = gen.uniform(size=batch_size)\n",
    "        X = torch.Tensor([featurize(z) for z in p])\n",
    "        minimax.addobs((p, X))\n",
    "        if n & (n - 1) == 0:\n",
    "            with torch.no_grad():\n",
    "                p = np.array([ z/10 for z in range(11) ])\n",
    "                X = torch.Tensor([ featurize(z) for z in p ])\n",
    "                Q, cons = primal_player.predict((p, X))\n",
    "                dual = dual_player.predict(peek=-cons)\n",
    "                lam = primal_player._iwmart.getlam()\n",
    "                beta, _ = primal_player._iwmart.getbeta()\n",
    "                print(f'{n:5d}\\t', f'{lam:.03e}\\t', f'{beta:.03e}\\t', f'{p}\\t', f'{Q.numpy()}\\t', f'{cons:.3e}\\t', f'{dual:.3e}\\t')\n",
    "\n",
    "def test():\n",
    "    import numpy as np\n",
    "\n",
    "    gen = np.random.default_rng(9)\n",
    "    test_once(gen)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea55f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
